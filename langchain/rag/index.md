# RAG 问答
## 概述
由大型语言模型（LLMs）支持的问答（Q&A）聊天机器人是最强大的应用之一。这些应用能够回答关于特定来源信息的提问。这些应用使用一种名为检索增强生成（Retrieval Augmented Generation，简称 RAG）的技术。
## 什么是 RAG？
RAG 是一种用额外数据增强 LLM 知识的技术。
LLMs 能够对广泛的话题进行推理，但其知识仅限于训练时使用到的公共数据。如果您想构建能够对私有数据或模型截止日期后引入的数据进行推理的 AI 应用，您需要用特定的信息来增强模型的知识。将相关信息引入模型提示的过程称为检索增强生成（RAG）。
LangChain 有多个组件旨在帮助构建问答应用，以及更一般的 RAG 应用。
> 注意：这里我们关注的是非结构化数据的问答。我们在其他地方介绍的两个 RAG 用例是：
>
> - SQL 数据的问答
> - 代码（例如 Python）的问答
## RAG 架构
一个典型的 RAG 应用有两个主要组成部分：
- **索引**：一个从源数据中摄取数据并进行索引的管道。这通常是在离线状态下进行的。
- **检索与生成**：实际的 RAG 链，它在运行时接收用户查询，从索引中检索相关数据，然后将这些数据传递给模型。
从原始数据到答案的最常见完整序列如下：
### 索引
1. **加载**：首先我们需要加载数据。这是通过 DocumentLoaders 完成的。
2. **拆分**：文本拆分器将大文档拆分成更小的片段。这对于索引数据和传递给模型都很有用，因为大片段更难搜索，并且无法适应模型的有限上下文窗口。
3. **存储**：我们需要一个地方来存储和索引我们的拆分片段，以便稍后可以搜索它们。这通常使用 VectorStore 和 Embeddings 模型来完成。
![index_diagram](../static/img/rag_indexing.png)
### 检索与生成
1. **检索**：给定用户输入，使用检索器从存储中检索相关的拆分片段。
2. **生成**：ChatModel / LLM 使用包括问题和检索到的数据的提示来生成答案。
![index_diagram](../static/img/rag_retrieval_generation.png)
## 目录
- [快速开始](#快速开始)
  - 我们建议从这里开始。以下许多指南都假定您完全理解快速开始中展示的架构。
- [返回源文档](#返回源文档)
  - 如何返回在特定生成中使用的源文档。
- [流式处理](#流式处理)
  - 如何将最终答案以及中间步骤进行流式处理。
- [添加聊天历史](#添加聊天历史)
  - 如何在问答应用中添加聊天历史。
- [按用户检索](#按用户检索)
  - 当每个用户都有他们自己的私有数据时，如何进行检索。
- [使用代理](#使用代理)
  - 如何在问答中使用代理。
- [使用本地模型](#使用本地模型)
  - 如何在问答中使用本地模型。
## 快速开始
LangChain 提供了多种组件，旨在帮助构建问答应用，以及更一般的 RAG 应用。为了熟悉这些组件，我们将构建一个简单的文本数据源的问答应用。在这个过程中，我们将介绍典型的问答架构，讨论相关的 LangChain 组件，并突出更高级问答技术的附加资源。我们还将看到 LangSmith 如何帮助我们追踪和理解我们的应用。随着应用复杂性的增加，LangSmith 将变得越来越有帮助。
这些组件包括：
- **DocumentLoaders**：用于从不同数据源加载数据的组件。
- **Text Splitters**：用于将大文档拆分成更小的片段的组件。
- **Embeddings Model**：用于将文本转换为向量的工具。
- **VectorStore**：用于存储文档的向量表示的数据库。
- **Retrievers**：用于根据用户输入和文档的向量表示找到最相关的文档片段的组件。
- **ChatModel / LLM**：用于生成答案的大型语言模型。

### 设置
#### 依赖项
在这个演练中，我们将使用 OpenAI 的聊天模型和嵌入以及 Chroma 向量存储，但这里展示的一切都适用于任何聊天模型（ChatModel）、大型语言模型（LLM）、嵌入和向量存储（VectorStore）或检索器。
我们将使用以下包：
```bash
pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4 pypdf
```

### OpenAI 设置
我们需要设置环境变量 `OPENAI_API_KEY`，推荐从 `.env` 文件中加载，如下所示：
```
OPENAI_API_KEY="sk-xxx"
# OPENAI_API_BASE="https://api.openai.com/v1"
```
### LangSmith
使用 LangChain 构建的应用程序通常包含多个步骤，涉及多次调用大型语言模型（LLM）。随着这些应用程序变得越来越复杂，能够检查链或代理内部确切发生的事情变得至关重要。最好的方法是使用 LangSmith。

请注意，LangSmith 并非必需，但它非常有帮助。如果您确实想使用 LangSmith，请在上述链接注册后，确保设置您的环境变量以开始记录跟踪信息：
```
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=xxx
```

## 返回源文档
## 流式处理
## 添加聊天历史
## 按用户检索
## 使用代理
## 使用本地模型
